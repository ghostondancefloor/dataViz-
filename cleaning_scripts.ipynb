{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération de la metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fr.sputniknews.africa--20220630--20230630.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def flatten_all(metadata, keys_to_keep=None):\n",
    "    all_data = metadata.get('all', {})\n",
    "    if keys_to_keep:\n",
    "        all_data = {key: all_data[key] for key in keys_to_keep if key in all_data}\n",
    "    all_df = pd.json_normalize(all_data, sep='_')\n",
    "    all_df['level'] = 'all'\n",
    "    return all_df\n",
    "\n",
    "def flatten_year_data(year, year_info, keys_to_keep=None):\n",
    "    year_flat = []\n",
    "    for category, values in year_info.items():\n",
    "        if keys_to_keep and category not in keys_to_keep:\n",
    "            continue  \n",
    "        if isinstance(values, dict): \n",
    "            category_df = pd.DataFrame(list(values.items()), columns=['key', 'value'])\n",
    "        else:\n",
    "            category_df = pd.DataFrame({'key': [category], 'value': [values]})\n",
    "        category_df['year'] = year\n",
    "        category_df['category'] = category\n",
    "        year_flat.append(category_df)\n",
    "    return pd.concat(year_flat, ignore_index=True)\n",
    "\n",
    "def flatten_month_data(year, month, month_info, keys_to_keep=None):\n",
    "    month_flat = []\n",
    "    for category, values in month_info.items():\n",
    "        if keys_to_keep and category not in keys_to_keep:\n",
    "            continue\n",
    "        if isinstance(values, dict): \n",
    "            category_df = pd.DataFrame(list(values.items()), columns=['key', 'value'])\n",
    "        else:\n",
    "            category_df = pd.DataFrame({'key': [category], 'value': [values]})\n",
    "        category_df['year'] = year\n",
    "        category_df['month'] = month\n",
    "        category_df['category'] = category\n",
    "        month_flat.append(category_df)\n",
    "    return pd.concat(month_flat, ignore_index=True)\n",
    "\n",
    "keys_to_keep = ['kws', 'loc', 'org' ]  \n",
    "\n",
    "all_df = flatten_all(data.get('metadata', {}), keys_to_keep=keys_to_keep)\n",
    "all_df.to_csv('metadata/all_data_chunk.csv', index=False)  \n",
    "\n",
    "yearly_dfs = []\n",
    "for year, year_info in data.get('metadata', {}).get('year', {}).items():\n",
    "    yearly_df = flatten_year_data(year, year_info, keys_to_keep=keys_to_keep)\n",
    "    yearly_df.to_csv(f'metadata/year_data_{year}_chunk.csv', index=False)\n",
    "\n",
    "monthly_dfs = []\n",
    "for year, months in data.get('metadata', {}).get('month', {}).items():\n",
    "    for month, month_info in months.items():\n",
    "        monthly_df = flatten_month_data(year, month, month_info, keys_to_keep=keys_to_keep)\n",
    "        monthly_df.to_csv(f'metadata/month_data_{year}_{month}_chunk.csv', index=False)  \n",
    "\n",
    "print(\"Data processing complete. Chunks saved as individual CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"metadata\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.startswith('year_data_2022')]\n",
    "\n",
    "dataframes = [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files]\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_file = \"metadata/year_data_2022.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All CSV files merged into {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération de la data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fr.sputniknews.africa--20220630--20230630.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "def flatten_data(year, month, day, day_data):\n",
    "    \"\"\"\n",
    "    Flatten the nested structure of the 'data' section into a DataFrame.\n",
    "    \"\"\"\n",
    "    if isinstance(day_data, list):\n",
    "        day_df = pd.DataFrame(day_data)\n",
    "        day_df['year'] = year\n",
    "        day_df['month'] = month\n",
    "        day_df['day'] = day\n",
    "        return day_df\n",
    "    else:\n",
    "        print(f\"Unexpected data format for {year}-{month}-{day}: {day_data}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_data(data, output_dir='output_data', chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Process the 'data' section and save it as CSV chunks.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True) \n",
    "    for year, months in data.get('data', {}).items():\n",
    "        for month, days in months.items():\n",
    "            for day, day_data in days.items():\n",
    "                day_df = flatten_data(year, month, day, day_data)\n",
    "                \n",
    "                if not day_df.empty:\n",
    "                    num_chunks = (len(day_df) // chunk_size) + 1\n",
    "                    for i in range(num_chunks):\n",
    "                        chunk = day_df[i * chunk_size:(i + 1) * chunk_size]\n",
    "                        if not chunk.empty:\n",
    "                            output_file = os.path.join(output_dir, f'data_{year}_{month}_{day}_chunk{i + 1}.csv')\n",
    "                            chunk.to_csv(output_file, index=False)\n",
    "                            print(f\"Saved {output_file}\")\n",
    "\n",
    "process_data(data)\n",
    "\n",
    "print(\"Data processing complete. CSV chunks saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output_data'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"data\") and filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        dataframes.append(pd.read_csv(filepath))\n",
    "        os.remove(filepath)\n",
    "\n",
    "merged_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_filepath = os.path.join(directory, 'data.csv')\n",
    "merged_dataframe.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Merged file saved at: {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code pour récupérer les données de géolocalisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"metadata\\year_data_2022.csv\")\n",
    "location_df = df[df[\"category\"] == \"loc\"]\n",
    "\n",
    "\n",
    "def is_valid_location(location):\n",
    "    \"\"\"\n",
    "    Vérifie si une localisation est correcte.\n",
    "    \"\"\"\n",
    "    # Ignorer les NaN\n",
    "    if pd.isna(location) or isinstance(location, bool) or not isinstance(location, str):\n",
    "        return None\n",
    "\n",
    "    location = re.sub(r\"[\\\"“”„]\", \"\", location)\n",
    "    \n",
    "    if re.search(r\"\\d\", location): \n",
    "        return None\n",
    "    \n",
    "    if re.match(r\"^[0-9@#]\", location) or re.search(r\"\\d\", location):\n",
    "        return None\n",
    "    location = re.sub(r\"^(l'|la |le |les |de |du |des )\", \"\", location, flags=re.IGNORECASE)\n",
    "\n",
    "    if re.match(r\"^[0-9@#]\", location):\n",
    "        return None\n",
    "\n",
    "    if not re.match(r\"^[\\w\\s\\-À-ÿ]+$\", location): \n",
    "        return None\n",
    "    \n",
    "    if len(location.strip()) < 3:\n",
    "        return None\n",
    "\n",
    "    return location if location else None\n",
    "\n",
    "location_df[\"key\"] = location_df[\"key\"].apply(is_valid_location)\n",
    "\n",
    "location_df = location_df[location_df[\"key\"].notnull()]\n",
    "\n",
    "location_df.to_csv(\"localisation_2023.csv\", index=False)\n",
    "print(f\"Dataset nettoyé sauvegardé\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def get_geocode(location):\n",
    "    \"\"\"\n",
    "    Fetch geocoding data from openstreetmap API for a given location in French.\n",
    "    \"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\n",
    "        \"q\": location, \n",
    "        \"format\": \"json\",  \n",
    "        \"limit\": 1, \n",
    "        \"accept-language\": \"en\"  \n",
    "    }\n",
    "    headers = {\"User-Agent\": \"MyGeocoderApp/1.0\"}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data: \n",
    "                result = data[0]\n",
    "                lat = float(result.get(\"lat\", 0))\n",
    "                lon = float(result.get(\"lon\", 0))\n",
    "                display_name = result.get(\"display_name\", \"Inconnu\")  \n",
    "                country = display_name.split(\",\")[-1].strip() if \",\" in display_name else result.get(\"name\", \"Inconnu\")\n",
    "                return lat, lon, country\n",
    "        else:\n",
    "            print(f\"Erreur : Statut {response.status_code} pour la localisation {location}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération des données pour {location} : {e}\")\n",
    "    return None, None, \"Inconnu\"\n",
    "\n",
    "# Charger votre dataset\n",
    "df = pd.read_csv('localisation_2023.csv')\n",
    "df['latitude'] = None\n",
    "df['longitude'] = None\n",
    "df['country'] = None\n",
    "\n",
    "# Géocodage pour chaque localisation dans la colonne 'key'\n",
    "for index, row in df.iterrows():\n",
    "    location = row['key'].strip() \n",
    "    lat, lon, country = get_geocode(location)\n",
    "    time.sleep(1)  \n",
    "    df.at[index, 'latitude'] = lat\n",
    "    df.at[index, 'longitude'] = lon\n",
    "    df.at[index, 'country'] = country\n",
    "\n",
    "\n",
    "output_file = 'metadata/geocoded_locations_2023.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Données géocodées sauvegardées dans {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code pour compter combien de fois des organisations sont cités ensemble :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "keywords_file = 'metadata/year_data_2022.csv'  \n",
    "articles_file = 'output_data/data.csv'\n",
    "\n",
    "keywords_data = pd.read_csv(keywords_file)\n",
    "\n",
    "keywords_data = keywords_data[keywords_data['category'] == 'org']\n",
    "\n",
    "articles_data = pd.read_csv(articles_file)\n",
    "\n",
    "if 'org' not in articles_data.columns or articles_data['org'].isnull().all():\n",
    "    raise ValueError(\"The 'org' column is missing or empty in the articles file.\")\n",
    "\n",
    "try:\n",
    "    articles_data['org_dict'] = articles_data['org'].apply(eval)  \n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to parse 'org' column: {e}\")\n",
    "\n",
    "# Build a list of keyword pairs\n",
    "keyword_pairs = []\n",
    "\n",
    "for _, row in articles_data.iterrows():\n",
    "    keywords = list(row['org_dict'].keys())\n",
    "    keyword_pairs.extend(combinations(keywords, 2))\n",
    "\n",
    "# Convert to a DataFrame\n",
    "keyword_pairs_df = pd.DataFrame(keyword_pairs, columns=['keyword', 'related_keyword'])\n",
    "\n",
    "# Count the frequency of each pair (optional)\n",
    "keyword_pairs_count = keyword_pairs_df.value_counts().reset_index(name='count')\n",
    "\n",
    "# Save results to a CSV file\n",
    "keyword_pairs_count.to_csv('output_data/org_relationships.csv', index=False)\n",
    "print(\"Keyword relationships saved to 'keyword_relationships.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
